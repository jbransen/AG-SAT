\documentclass{llncs}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[T1]{fontenc}
\usepackage{tikz}
\usetikzlibrary{arrows, automata}
\input{./formats.tex}

% Paper info
\title{Linear Ordered Attribute Grammar scheduling using SAT-solving}
\author{Jeroen Bransen\inst{1}  \and L. Thomas van Binsbergen\inst{2,1} \and Koen Claessen\inst{3} \and Atze Dijkstra\inst{1}}
\institute{Utrecht University, Utrecht, The Netherlands, \email{\{J.Bransen,atze\}@uu.nl}
\and Royal Holloway, University of London, Egham, UK, \email{Thomas.VanBinsbergen.2014@live.rhul.ac.uk}
\and Chalmers University of Technology, Gothenburg, Sweden, \email{koen@chalmers.se}}

% Wat handige shortcuts
\newif\iffinal\finalfalse
\newcommand{\REM}[3]{\iffinal\else\textcolor{#2}{[#1: #3]}\fi}
\newcommand{\Atze}[1]{\REM{Atze}{red}{#1}}
\newcommand{\Koen}[1]{\REM{Koen}{red}{#1}}
\newcommand{\Jeroen}[1]{\REM{Jeroen}{red}{#1}}
\newcommand{\Thomas}[1]{\REM{Thomas}{red}{#1}}

\newcommand{\figref}[1]{Figure~\ref{#1}}
\newcommand{\tabref}[1]{Table~\ref{#1}}
\newcommand{\sectref}[1]{Section~\ref{#1}}

% Handle citing "Van Binsbergen" correctly...
\DeclareRobustCommand{\VAN}[3]{#2}

\begin{document}

\maketitle

\begin{abstract}
Many computations over trees can be specified using attribute
grammars. Compilers for attribute grammars need to find an evaluation
order (or schedule) in order to generate efficient code. For the class
of linear ordered attribute grammars such a schedule can be found
statically, but this problem is known to be NP-hard.

In this paper, we show how to encode linear ordered attribute grammar
scheduling as a SAT-problem. For such grammars it is necessary to ensure that the dependency graph is cycle free, which we approach in a novel way by transforming the dependency graph to a chordal graph allowing the cycle freeness to be efficiently expressed and computed using SAT solvers.

There are two main advantages to using a SAT-solver for scheduling:
(1) the scheduling algorithm runs much faster than existing scheduling
algorithms on real-world examples, and (2) by adding extra constraints
we obtain fine-grained control over the resulting schedule, thereby
enabling new scheduling optimisations.


\keywords{Attribute Grammars, static analysis, SAT-solving}
\end{abstract}

\section{Introduction}
Attribute Grammars \cite{knuth68} are a formalism for describing the semantics of context-free languages, thereby making them suitable for the construction of compilers. Examples of compilers written using attribute grammars are JastAddJ \cite{Ekman:2007}, a Java compiler, and UHC \cite{Dijkstra:2009}, a Haskell compiler. The use of attribute grammars in the UHC also motivates this paper and as such UHC is a real world test case for evaluating the effectiveness of the given approach.

An attribute grammar essentially describes computations over trees, also known as folds or catamorphisms. Although the attribute grammar defines exactly \emph{what} should be computed, it does not define \emph{when} values should be computed. It is therefore up to the attribute grammar compiler (the compiler that translates the attribute grammar definition into a runtime evaluator) to find an evaluation order. Such an evaluation order or schedule should satisfy the dependencies induced by the specification of attribute computations in the attribute grammar definition and must be found statically, which means that for the given grammar an evaluation order should be found such that for every finite abstract syntax tree of the grammar the evaluation order should compute all values in a finite number of steps. Specifically, no cyclic data dependencies may occur at runtime for any given parse tree.

The class of \emph{ordered attribute grammars} \cite{kastens80} is a well-known subclass of attribute grammars for which a polynomial time scheduling algorithm exists. However, despite what the name suggests there exist attribute grammars for which a static evaluation order can be found that do not belong to the class of ordered attribute grammars. In our work on compiler construction we often encountered this situation, giving rise to the need of other scheduling algorithms \cite{bransen2012,Binsbergen:2015}. We therefore look at the class of \emph{linear ordered attribute grammars} in this paper, which is the largest class of attribute grammars for which a static evaluation order can be found. The problem of statically finding such evaluation order is known to be NP-complete \cite{engelfriet82}.

\subsection{Summary}
We solve the linear ordered attribute grammar scheduling problem by translating it into the Boolean satisfiability problem (SAT), one of the best-known NP-complete problems \cite{Cook:1971}. The SAT problem has been the subject of a lot of research and even though the worst case runtime of all known SAT-solving algorithms is exponential in the input size, many SAT-solvers work very well in practice. \Jeroen{todo: referenties hier, misschien naar SAT-solving overzicht?} By translating into the SAT problem we can therefore use an efficient existing SAT-solver to solve our problem, and even benefit from future improvements in the SAT-community. In our implementation we use MiniSat\footnote{\url{http://minisat.se}} \cite{Een:2004}.

The core of the scheduling problem consists of finding a total order on the nodes of a set of dependency graphs (directed acyclic graphs) such that all direct dependencies, which are coming from the input grammar, are satisfied. However, there is interplay between the different dependency graphs caused by the order that is found in one dependency graph resulting in indirect dependencies that need to be satisfied in another dependency graph. This interplay of dependencies, which is explained later in detail, is what makes the problem hard.

To encode this problem in SAT we represent each edge in the dependency graphs as a Boolean variable, with its value indicating the direction of the edge. For the direct dependencies the value is already set, but for the rest of the variables the SAT-solver may choose the direction of the edge. Ensuring cycle freeness requires us to encode transitivity with SAT-constraints, which in the straight-forward solution leads to a number of extra constraints cubic in the number of variables. To avoid that problem we make our graphs \emph{chordal} \cite{Dirac:1961}. In a chordal graph every cycle of size $> 3$ contains an edge between two non-adjacent nodes in the cycle. In other words, if there exists a cycle in the graph, there must also exist a cycle of at most three nodes. This allows us to encode cycle freeness much more efficiently by only disallowing cycles of length three.

Apart from the fact that this translation into the SAT problem helps in efficiently (in practice) solving the scheduling problem, there also is another benefit: it is now possible to encode extra constraints on the resulting schedule. We show two scheduling optimisations that are interesting from an attribute grammar point of view, for which the optimal schedule can be found efficiently by expressing the optimisation in the SAT problem.

\subsection{Overview}
In this paper we make the following main contributions:
\begin{itemize}
\item We show how to encode the problem of scheduling linear ordered attribute grammars as a SAT problem
\item We show that chordal graphs can be used to encode cycle freeness in SAT problems
\item We show how certain attribute grammar optimisations can be encoded as part of the formulation of the SAT problem
\end{itemize}
Furthermore, we have implemented the described techniques in the UUAGC\footnote{\url{http://www.cs.uu.nl/wiki/HUT/AttributeGrammarSystem}} \cite{combinator-languages} and show that the technique works well in practice for the UHC.

The outline of the paper is as follows. We start in \sectref{sect:ag} and \sectref{sect:loag} by explaining linear ordered attribute grammars and the difficulties in scheduling in more detail. We then describe the translation into the SAT problem in \sectref{sect:translation} and show the effectiveness of our approach in \sectref{sect:results}. In \sectref{sect:optimisations} we describe the two attribute grammar optimisations and finally in \sectref{sect:conclusion} we discuss some problems and conclude.


\section{Attribute Grammars} \label{sect:ag}
An attribute grammar consists of three parts: a context-free grammar, a set of attribute definitions and a set of semantic functions. Instead of a context-free grammar describing the concrete syntax of the language, we feel that it is more intuitive to visualise the grammar as describing the abstract syntax tree of the language. We therefore say that an attribute grammar consists of a set of algebraic data types describing the abstract syntax, a set of attribute definitions and a set of semantic functions. The attribute definitions and semantic functions describe how values should be computed over the abstract syntax tree.

In the rest of this paper we stick to the usual attribute grammar terminology, so with a \emph{nonterminal} we mean a type, and a \emph{production} is a constructor of that type. Furthermore, the name \emph{lhs} (short for left-hand side) refers to the constructor itself, while the children of a production have separate names. As this paper is about scheduling and data dependencies, we do not explain the syntax of the semantic functions here, but it is important to remark that these functions define how the value of a certain attribute can be computed from other attributes.

There are two types of attributes: \emph{inherited} attributes with values that are passed from a parent to its children, and \emph{synthesized} attributes with values that are passed from the children to their parents. In the pictures we draw inherited attributes on the left side of a node, and the synthesized attributes on its right side.

\mydoublepic{labval_bin_flow_full}{0.55}{labval_leaf_flow_full}{0.3}{
    Example attribute grammar for binary trees with two inherited and two synthesized attributes}

In \figref{fig:labval_bin_flow_full} we show an example attribute grammar with one nonterminal \emph{Tree} which has two productions: \emph{Bin} and \emph{Leaf}. The \emph{Bin} production has two children named \emph{l} and \emph{r}, both of the type \emph{Tree}, and \emph{Leaf} has no children. The \emph{Tree} nonterminal has four attributes: two inherited attributes a1 and a2 and two synthesized attributes a3 and a4.

The figure shows what is called the \emph{production dependency graphs}. For a production two different types of attributes can be distinguished: \emph{input} attributes, which are attributes for which the value is provided, and \emph{output} attributes, which are values that need to be defined as part of the production relating parent and children. The input attributes of a production, coloured gray in the picture, are the inherited attributes of the production itself, because their values are provided by the parent node, and the synthesized attributes of all its children. The output attributes, coloured white in the picture, are the synthesized attributes of the parent and the inherited attributes of the child.

Although we talk about dependency graphs in the context of scheduling, we actually draw the edges in the other direction. The edges in the picture represent data flow, which from an attribute grammar perspective is much more intuitive. For example, in the \emph{Bin} production the attribute a1 of the child \emph{r} is computed from a1 from the parent and a3 from the child \emph{l}. The edges are thus always directed from an input attribute to an output attribute, and the actual dependency graph can be obtained by reversing all edges.

\section{Linear Ordered Attribute Grammars} \label{sect:loag}
Given an attribute grammar definition the attribute grammar compiler generates a runtime evaluator that takes an abstract syntax tree as input and computes all attribute values. There are two approaches for doing the scheduling: at runtime (dynamic) or at compile time (static).

For dynamic scheduling one could rely on existing machinery for lazy evaluation, for example in Haskell, which is the approach taken by \cite{saraiva99}. There the attribute grammar definitions are translated into lazy Haskell functions in a straightforward way by producing functions that take the inherited attributes as argument and return the synthesized attributes. Whenever an inherited attribute (indirectly) depends on the value of a synthesized attribute, this means the function has a cyclic definition, which is no problem for languages with lazy semantics, and can actually result in efficient code \cite{Bird:1984}.

There are two problems with the dynamic scheduling. First, whenever the attribute grammar contains a true cyclic attribute definition, the evaluator enters an infinite loop at runtime. However, these cycles could have been detected at compile time! Furthermore, the code needs lazy functions while with the static scheduling strict evaluators can be generated, leading to more efficient code in practice.

For static scheduling efficient evaluators can be generated, but the problem is that static scheduling is hard. In \cite{kastens80} a polynomial time algorithm is given for static scheduling of the class of ordered attribute grammars, but unfortunately we often encounter attribute grammars outside of that class. All our attribute grammars do however fall in the class of linear ordered attribute grammars, which is the largest class of attribute grammars for which a static schedule can be found. Although the scheduling is NP-complete, we have implemented a backtracking algorithm in earlier work \cite{Binsbergen:2015} that is feasible in practice. However, in this paper we show that we can do better by creating an algorithm that is faster and allows for different optimisations on the resulting schedule.

Conceptually, static scheduling of linear ordered attribute grammars is not very complex. For each nonterminal a total order should be found on its attributes, such that all production dependency graphs all cycle free. However, because a nonterminal can have multiple productions and a production can have children of different types, choices made on the order of attributes of one nonterminal can influence the order of attributes in another nonterminal.

In order to encode this we also construct a \emph{nonterminal dependency graph}. This graph contains all attributes that are defined for the corresponding nonterminal and the edges in this graph must define a total order on the attributes. Furthermore, the edges in the nonterminal dependency graph should agree with the (indirect) dependencies from the production dependency graphs such that no cycles exist.

\mypic{labval_bin_flow_induced}{Production dependency graph for Bin for the order a2 $\rightarrow$ a4 $\rightarrow$ a1 $\rightarrow$ a3}

\figref{fig:labval_bin_flow_induced} shows the production dependency graph of the production \emph{Bin} with a complete order on the attributes. In this case there is only a single nonterminal so both the parent and the child nodes have the same attribute order, made explicit by extra edges from the nonterminal dependency graph. Because this dependency graph is still cycle free after adding the edges for the complete order and the same holds for the \emph{Leaf} production, the order a2 $\rightarrow$ a4 $\rightarrow$ a1 $\rightarrow$ a3 is a valid schedule for the nonterminal \emph{Tree}.


\section{Translation into SAT} \label{sect:translation}
To represent the scheduling problem as a Boolean formula we introduce a variable for each edge, indicating the direction of the edge. The direct dependencies coming from the source code are constants, but for the rest of the edges the SAT-solver can decide on the direction. However, the encoding has been chosen in such way that a valid assignment of the variables corresponds to a valid schedule.

Our algorithm has the following steps:
\begin{enumerate}
\item Construct a nonterminal dependency graph for each nonterminal
\item Construct a production dependency graph for each production
\item Make all graphs chordal
\item Introduce a SAT variable for each edge in any of the graphs, where edges from a production dependency graph and the corresponding nonterminal dependency graph are shared, so are represented by the same variable
\item Set the value of all variables corresponding to direct dependencies
\item Exclude all cycles of length three
\item Optionally add extra constraints for optimisations
\end{enumerate}
The first two steps have been explained in the previous sections. Step 3 is explained below, step 4 is trivial, and step 5 and 6 follow from the explanation below. Finally step 7 is explained in \sectref{sect:optimisations}.

\subsection{Chordal graphs}
A chordal graph is an undirected graph in which each cycle of length $> 3$ contains a \emph{chord}. A chord is an edge between two nodes in the cycle that are not adjacent. As a consequence each cycle of length $> 3$ can be split up into two smaller cycles, so if a chordal graph contains a cycle it must also contain a cycle of size three. Chordal graphs are therefore sometimes also referred to as \emph{triangulated graphs}.

\mypic{simplechord}{There can not exists a cycle w-n-e-s without either
a cycle n-s-w or n-e-s.}

In our case the graphs are directed, but we can still apply the same trick! In \figref{fig:simplechord} we illustrate how to use chordal graphs to exclude cycles. If there exists a cycle of length four, then it is not possible to choose a direction for the edge between \emph{n} and \emph{s} without introducing a cycle of length three. Hence, if we make our graph chordal by adding edges which may have an arbitrary direction and explicitly exclude all cycles of length three, we ensure that no cycles can exist at all.

\subsection{Chordal graph construction} \label{sect:chordalconstruct}
There are several algorithms for making a graph chordal. We use an algorithm based on the following alternative definition of a chordal graph:

\begin{definition}
An undirected graph is \emph{chordal} if and only if it has a 
perfect elimination order. A perfect elimination order is an
ordering $v_1,\ldots,v_n$ of the vertices of $G$ such that in the graph
$G[v_1,\ldots,v_i]$, $\univ{i}{1 \leq i \leq n}$, the vertex $v_i$ is 
\toindex{simplicial}. A vertex $v$ is called simplicial in a graph $G$ if
the neighbourhood of $v$ forms a connected component in $G$.
The graph $G[v_1,\ldots,v_i]$ is the induced subgraph of $G$ containing
only the vertices $v_i,\ldots,v_i$ and the edges between these vertices.
\end{definition}

From this definition we can construct the following algorithm for making a graph chordal:
\begin{enumerate}
 \item While the graph still contains vertices:
 \begin{enumerate}
     \item Select a vertex $v$ from the graph
     \item For every pair $(a,b)$ of unconnected vertices in the neighbourhood
        of $v$: 
        \begin{enumerate}
            \item Add the edge $(a\leftrightarrow b)$ to the graph
        \end{enumerate}
     \item Remove $v$, and all edges connected to $v$, from the graph.
 \end{enumerate}
\end{enumerate}

One important open question in this algorithm is the order in which the vertices should be chosen. In \sectref{sect:heuristics} we show the results for several heuristics that we have implemented and tried on the UHC. We would like to remark that regardless of the heuristic used, this approach always leads to much smaller SAT problems than encoding transitivity in the SAT problem for ruling out cycles.

\subsection{Finding the schedule}
When the constructed Boolean formula is given to the SAT-solver, the result is either that the formula is not satisfiable, meaning that no schedule has been found, of satisfiable, meaning that there is a schedule. It is not hard to see that the formula is satisfiable if and only if there exists a valid schedule for the given attribute grammar definition, and in this paper we give no formal proof of this claim.

In the case where the formula is satisfiable, we obviously want to find the result. From the SAT solver we can ask for the truth value of each variable in the solution, so when our algorithm keeps the connection between edges and variables we can complete our directed graph and trivially find the complete order for all attributes from that. The constraints guarantee that this graph contains no cycles.

\subsection{Shared edges}
One important implementation detail is that of shared edges. As explained, the nonterminal dependency graphs and the production dependency graphs share the edges that define the order of the attributes. Because each edge is represented by a variable in the SAT problem we can simply encode this by assigning the same variable to the shared edges.

However, as we also make both graphs chordal, the edges added to make the graphs chordal can also be shared. This is exactly what our implementation does, such that the SAT problem is kept as small as possible. The implementation is therefore slightly more complicated than explained in the previous sections.

\section{Empirical results} \label{sect:results}
We use the Utrecht Haskell Compiler (UHC) \cite{Dijkstra:2009} as a test case for our work. The source code of the UHC consists of several attribute grammar definitions together with Haskell code. The biggest attribute grammar in the UHC, called \emph{MainAG}, consists of 30 nonterminals, 134 productions, 1332 attributes (44.4 per nonterminal) and 9766 dependencies and is the biggest attribute grammar we know of.

The UHC is compiled using the Utrecht University Attribute Grammar Compiler (UUAGC) \cite{combinator-languages}, in which we have implemented our approach. Apart from our SAT approach there are three other scheduling algorithms that have been implemented to which we can compare our approach:

\begin{itemize}
\item \emph{Kastens}: the algorithm from \cite{kastens80} which only works for ordered attribute grammars, of which MainAG is not a member. However, with extra manual annotations this algorithm can generate a schedule, which is the approach used previously. However, as writing these annotation is hard an error-prone, this is not an optimal work flow.
\item \emph{Kennedy-Warren}: The algorithm from \cite{kennedywarren76} that we have implemented in the UUAGC before \cite{bransen2012} is an algorithm that schedules all absolutely noncircular attribute grammars, an even larger class than the linear ordered attribute grammars. However, the scheduling is not completely static so the generated evaluator also does part of the scheduling at runtime. 
\item \emph{LOAG-backtrack}: we have also implemented a backtracking algorithm for linear ordered attribute grammars \cite{Binsbergen:2015}, based on Kastens' algorithm. This algorithm solves exactly the same scheduling problem as the SAT approach described in this paper and uses exponential time in worst case.
\end{itemize} 

\begin{table}
  \begin{center}
    \begin{tabular}{l || c || r}
      Algorithm   &   Manual annotations   &  Compile time MainAG \\
      \hline
      Kastens'                &   Y           &   17s \\
      Kennedy-Warren  &   N           &   33s \\
      LOAG-backtrack  &   N           &   36s \\
      LOAG-SAT           &   N           &   9s \\
    \end{tabular}
  \end{center}
  \caption{Comparison of the four scheduling algorithms}
  \label{tab:algo-comparison}
\end{table}

In \tabref{tab:algo-comparison} we show the compilation times for the MainAG of the UHC for each of the four different algorithms. All times include parsing of the attribute grammar description and code generation, which is what takes most time in the SAT approach. The SAT-solver takes less than a second to find a solution.

\subsection{Chordal graph heuristics} \label{sect:heuristics}
\begin{table}
  \begin{center}
    \begin{tabular}{c r r r }
      Order                                   &   \#Clauses   & \#Vars       & Ratio \\
      \hline
      $(\mid\mathcal{D}\mid,\mid\mathcal{S}\mid,\mid\mathcal{C}\mid)$ &  21,307,812  & \quad 374,792           & \quad 57.85 \\
      $(\mid\mathcal{D}\mid,\mid\mathcal{C}\mid,\mid\mathcal{S}\mid)$ &   8,301,557   & 220,690           & 37.62 \\
      $(\mid\mathcal{S}\mid,\mid\mathcal{D}\mid,\mid\mathcal{C}\mid)$ &  12,477,519   & 287,151           & 43.45 \\
      $(\mid\mathcal{S}\mid,\mid\mathcal{C}\mid,\mid\mathcal{D}\mid)$ &   8,910,379   & 241,853           & 36.84 \\
      $(\mid\mathcal{C}\mid,\mid\mathcal{D}\mid,\mid\mathcal{S}\mid)$ &   3,004,705   & 137,277           & 21.89 \\
      $(\mid\mathcal{C}\mid,\mid\mathcal{S}\mid,\mid\mathcal{D}\mid)$ &   3,359,910   & 156,795           & 21.43 \\
      $(\mid\mathcal{D}\mid+\mid\mathcal{S}\mid,\mid\mathcal{C}\mid)$     &  12,424,635   & 386,323           & 32.16 \\
      $(\mid\mathcal{D}\mid,\mid\mathcal{S}\mid+\mid\mathcal{C}\mid)$     &   8,244,600   & 219,869           & 37.50 \\
      $(\mid\mathcal{D}\mid+\mid\mathcal{C}\mid,\mid\mathcal{S}\mid)$     &   2,930,922   & 135,654           & 21.61 \\
      $(\mid\mathcal{S}\mid,\mid\mathcal{D}\mid+\mid\mathcal{C}\mid)$     &   8,574,307   & 236,348           & 36.28 \\
      $(\mid\mathcal{S}\mid+\mid\mathcal{C}\mid,\mid\mathcal{D}\mid)$     &   3,480,866   & 157,089           & 22.16 \\
      $(\mid\mathcal{C}\mid,\mid\mathcal{D}\mid+\mid\mathcal{S}\mid)$     &   3,392,930   & 157,568           & 21.53 \\
      $(\mid\mathcal{C}\mid+\mid\mathcal{D}\mid+\mid\mathcal{S}\mid)$     &   3,424,001   & 148,724           & 23.02 \\
      $(3*\mid\mathcal{S}\mid*(\mid\mathcal{D}\mid+\mid\mathcal{C}\mid)+(\mid\mathcal{D}\mid*\mid\mathcal{C}\mid) ^2)$     &   \underline{2,679,772}   & \underline{127,768}           & \underline{20.97} \\ \\
    \end{tabular}
  \end{center}
  \caption{Table showing the number of clauses and variables required for solving the MainAG of the UHC, selecting the next vertex in the elimination order based on different ways to compare neighbourhoods.}
  \label{tab:edgecomp}
\end{table}

As explained in \sectref{sect:chordalconstruct} we need to find an order in which to handle the vertices such that the resulting SAT problem is as small as possible. In \tabref{tab:edgecomp} we show the results of different heuristics for the MainAG of the UHC. In this table we use three different sets: $\mathcal{D}$ is the set of direct dependencies, $\mathcal{C}$ is the set of edges that are added to make the graph chordal and $\mathcal{S}$ is the set of other edges. For each of the sets we take only the edges in the neighbourhood of \emph{v} for comparison.

\section{Optimisations} \label{sect:optimisations}
We have shown that expressing the scheduling problem as a SAT problem and using an existing SAT-solver can improve the running time of the scheduling, but that is not the only advantage. In the SAT problem one can easily add extra constraints to further influence the resulting schedule. In \sectref{sect:minimising} and \sectref{sect:eagerattributes} we show two of such optimisations that are useful from an attribute grammar perspective. These optimisations have not been implemented in the release version of the UUAGC, but we have run preliminary experiments to verify that they work as expected.

\subsection{Interacting with the solver}
Instead of directly expressing all constraints in the initial SAT problem, we use a different trick for implementing the two optimisations: interacting with the solver. After the initial scheduling problem has been solved, we can ask for the truth value of all variables to construct the schedule. MiniSat also keeps some state in memory that allows us to add extra constraints to the problem, and ask for a new solution. In this way we can start with an initial solution and interact with the solver until some optimum has been reached.

\subsection{Minimising visits} \label{sect:minimising}
The result of the static scheduling is a runtime evaluator that computes the values of the attributes for a given abstract syntax tree. The total order for each nonterminal defines in what order attributes should be computed, but in the implementation of the evaluator we make use of a slightly bigger unit of computation: a \emph{visit}.

A visit is an evaluation step in the runtime evaluator that takes the values of a (possibly empty) set of inherited attributes and produces a (non-empty) set of synthesized attributes. In order to compute these values, visits to the child nodes may happen, and at the top of the tree the wrapping code invokes all visits of the top node one by one.

Because invoking a visit at runtime may have a certain overhead, we would like the number of visits to be as small as possible. In other words, in the total order on the attributes we would like to minimise the number of places where a synthesized attribute is followed by an inherited attribute, because that is the location where a new visit needs to be performed.

It is theoretically impossible to minimise the total number of visits performed for the full abstract syntax tree, because at compile-time we do not have a concrete abstract syntax tree at hand and only know about the grammar. We therefore try to minimise the maximum number of visits for any nonterminal, which is the number of alternating pairs of inherited and synthesized attributes in the total order.

We use the following algorithm:
\begin{enumerate}
\item Construct the initial SAT problem and solve
\item Repeat while there is a solution:
\begin{enumerate}
\item Find the longest chain of alternating inherited/synthesized attributes (which is the maximum number of visits)
\item If this maximum is lower then the previous best, store this result
\item Add a constraint stating that at least one of the edges in this chain must be reversed
\item Call solve again
\end{enumerate}
\end{enumerate}

Remark that removing the longest chain of visits might lead to a worst schedule, as this could result in other nonterminals using even more visits. We therefore need to store the best result found so far separately. However, it is guaranteed that this procedure will find the optimal result, and experiments have shown it usually does so quickly.

\subsection{Eager attributes} \label{sect:eagerattributes}
Another optimisation is the ability to define \emph{eager attributes}. Eager attributes are attributes that should be computed as soon as possible, and must be annotated by the attribute grammar programmer as such. We would like our scheduling algorithm then to schedule them as early as possible in the total order.

As an example, in a typical compiler there is an attribute containing the errors that occur in compilation. When running the compiler one is typically first interested in knowing if there are any errors; if so they must be printed to the screen and the compiler can stop its compilation. If there are no errors, then all other work that is not strictly necessary for the generation of errors can be done to complete the compilation.

We can use the following algorithm to implement this:
\begin{enumerate}
\item Construct the initial SAT problem and solve
\item Create a set $\mathcal{E}$ with the eager attributes
\item While $\mathcal{E}$ is not empty:
\begin{enumerate}
\item Take an attribute $\mathcal{A}$ from the set $\mathcal{E}$
\item For all dependencies of $\mathcal{A}$, add constraint that at least one the edges must be reverted
\item Solve, and if there is a solution:
\begin{enumerate}
\item Add $\mathcal{A}$ again to the set $\mathcal{E}$
\end{enumerate}
\item If there is no solution:
\begin{enumerate}
\item Revert to the previous solution (MiniSat allows this)
\item Add all dependencies of $\mathcal{A}$ to $\mathcal{E}$
\end{enumerate}
\end{enumerate}
\end{enumerate}

One important remark here is that $\mathcal{E}$ does not necessarily contain attributes from the same nonterminal. If at the root of the abstract syntax tree we would like to have an attribute depend on as few other attributes as possible, it may be the case that deeper in the tree the attributes need to be reordered in order to achieve that goal. The dependencies of $\mathcal{A}$ therefore means the dependencies in a production nonterminal graph, which can include attributes of other nonterminals (because children of a production can be of different nonterminals).

In contrast to the previous optimisation, this algorithm does not find a global optimum. Because optimistic choices are made this algorithm finds a local minimum, which may be much worse than the global optimum. We have not been able to verify what the influences of this are.

\section{Conclusion and discussion} \label{sect:conclusion}
We have explained the difficulties in attribute grammar scheduling and shown how to solve this problem using a SAT-solver. The given approach has been implemented and tested on a full-scale compiler built using attribute grammars. Results show that the algorithm works much faster than other approaches.

In the translation into the SAT-problem we have used a novel technique for ruling out cycles in the SAT problem using chordal graphs. This technique makes the problems much smaller than directly ruling out cycles, while encoding the same restrictions. We believe that this technique is applicable in other problems using SAT-solvers as well.

Furthermore, we have shown that expressing the problem as a SAT problem has the advantage that extra constraints can be added. We illustrated this with two possible properties of the resulting schedule that an attribute grammar programmer may want to influence. Even though this makes the scheduling problem potentially harder, as the algorithm is left fewer choices, the solution is found very fast in all practical cases we have tried.

Another benefit of this approach that the attribute grammar scheduling can benefit from breakthroughs in the SAT community. The more efficient SAT solvers become, the better the attribute grammar scheduling becomes leading to larger and larger attribute grammars that are feasible to schedule.

One problem with the current approach in contrast to most other scheduling algorithms is the unpredictability. SAT-solvers use certain heuristics to quickly find solutions in many applications, but it can theoretically happen that for a certain attribute grammar the SAT problem that is generated is not suitable for these heuristics. A seemingly innocent change in the attribute grammar definition could therefore theoretically lead to a large increase in compile time. However, we have not encountered this problem and we believe that this situation is very unlikely to happen because of the maturity of the SAT-solvers.

All in all, we believe that this approach fully solves the basic scheduling problem in an elegant way. There are ample possibilities for improving the resulting schedules based on attribute grammar knowledge like the two discussed in \sectref{sect:optimisations}, so we have also made room for future improvements in the scheduling and compilation of attribute grammars.

\DeclareRobustCommand{\VAN}[3]{#3}
\bibliographystyle{apalike}
\bibliography{biblio}

\end{document}
