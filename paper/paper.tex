\documentclass{llncs}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[T1]{fontenc}
\usepackage{tikz}
\usetikzlibrary{arrows, automata}
\input{./formats.tex}

% Paper info
\title{Linear Ordered Attribute Grammar scheduling using SAT-solving}
\author{Jeroen Bransen\inst{1}  \and L. Thomas van Binsbergen\inst{2,1} \and Koen Claessen\inst{3} \and Atze Dijkstra\inst{1}}
\institute{Utrecht University, Utrecht, The Netherlands, \email{\{J.Bransen,atze\}@uu.nl}
\and Royal Holloway, University of London, Egham, UK, \email{Thomas.VanBinsbergen.2014@live.rhul.ac.uk}
\and Chalmers University of Technology, Gothenburg, Sweden, \email{koen@chalmers.se}}

% Wat handige shortcuts
\newif\iffinal\finalfalse
\newcommand{\REM}[3]{\iffinal\else\textcolor{#2}{[#1: #3]}\fi}
\newcommand{\Atze}[1]{\REM{Atze}{red}{#1}}
\newcommand{\Koen}[1]{\REM{Koen}{red}{#1}}
\newcommand{\Jeroen}[1]{\REM{Jeroen}{red}{#1}}
\newcommand{\Thomas}[1]{\REM{Thomas}{red}{#1}}

\newcommand{\figref}[1]{Figure~\ref{#1}}
\newcommand{\sectref}[1]{Section~\ref{#1}}

% Handle citing "Van Binsbergen" correctly...
\DeclareRobustCommand{\VAN}[3]{#2}

\begin{document}

\maketitle

\begin{abstract}
Many computations over trees can be expressed using attribute
grammars. Compilers for attribute grammars need to find an evaluation
order (or schedule) in order to generate efficient code. For the class
of linear ordered attribute grammars such a schedule can be found
statically, but this problem is known to be NP-hard.

In this paper, we show how to encode linear ordered attribute grammar
scheduling as a SAT-problem. Cycle-freeness of the dependency graphs
is expressed by making the graphs chordal, which is a new approach for
encoding such graph properties in SAT.

There are two main advantages to using a SAT-solver for scheduling:
(1) the scheduling algorithm runs much faster than existing scheduling
algorithms on real-world examples\Thomas{Waarmee vergelijken we? 
Kastens algoritme is niet voor LOAGs, het voordeel dat onze aanpak geen 
augmenting dependencies nodig heeft zou dan in de vergelijking genoemd moeten 
worden}, and (2) by adding extra constraints
we obtain fine-grained control over the resulting schedule, thereby
enabling new scheduling optimisations.


\keywords{Attribute Grammars, static analysis, SAT-solving}
\end{abstract}

\section{Introduction}
Attribute Grammars \cite{knuth68} are a formalism for describing the semantics of context-free languages, thereby making them suitable for the construction of compilers. Examples of compilers written using attribute grammars are JastAddJ \cite{Ekman:2007}, a Java compiler, and UHC \cite{Dijkstra:2009}, a Haskell compiler. The UHC is also the motivation for this paper and is used as a real world test case for testing the effectiveness of the given approach.

An attribute grammar essentially describes computations over trees, also known as folds or catamorphisms. Although the attribute grammar defines exactly \emph{what} should be computed, it does not define \emph{when} values should be computed. It is therefore up to the attribute grammar compiler (the compiler that translates the attribute grammar definition into a runtime evaluator) to find an evaluation order. Such an evaluation order or schedule should satisfy the dependencies from the attribute grammar definition and must be found found statically, which means that for the given grammar an evaluation order should be found such that for every finite parse tree of the grammar the evaluation order should compute all values in a finite number of steps. Specifically, no cyclic data dependencies may occur at runtime for any given parse tree.

The class of \emph{ordered attribute grammars} \cite{kastens80} is a well-known subclass of attribute grammars for which a polynomial time scheduling algorithm exists. However, despite what the name suggests there exist attribute grammars for which a static evaluation order can be found that do not belong to the class of ordered attribute grammars. In our work on compiler construction we often encountered this situation, giving rise to the need of other scheduling algorithms \cite{bransen2012,Binsbergen:2015}. We therefore look at the class of \emph{linear ordered attribute grammars} in this paper, which is the largest class of attribute grammars for which a static evaluation order can be found. The problem of statically finding such evaluation order is known to be NP-complete \cite{engelfriet82}.

We solve the linear ordered attribute grammar scheduling problem by translating it to the Boolean satisfiability problem (SAT), one of the best-known NP-complete problems \cite{Cook:1971}. The SAT problem has been the subject of a lot of research and even though the worst case runtime of all known SAT-solving algorithms is exponential in the input size, many SAT-solvers work very well in practice. \Jeroen{todo: referenties hier, misschien naar SAT-solving overzicht?} By translating to SAT we can therefore use an efficient existing SAT-solver to solve our problem, and even benefit from future improvements in the SAT-community.

The core of the scheduling problem is essentially a set of dependency graphs (directed acyclic graphs) for which a total order on the nodes should be found such that all direct dependencies, which are coming from the input grammar, are satisfied. However, there is a certain interplay between the different dependency graphs resulting in indirect dependencies, coming from the order that is found in one dependency graph, that need to be satisfied another dependency graph. This interplay of dependencies, which is explained later in detail, is what makes the problem hard.

To encode this problem in SAT we represent each edge as a Boolean variable, with its value indicating the direction of the edge. For the direct dependencies the value is already set, but for the rest of the variables the SAT-solver may choose the direction of the edge. To ensure cycle-freeness this requires us to encode transitivity with SAT-constraints, which in the straight-forward solution leads to a number of extra constraints cubing in the number of variables. To avoid that problem we make our graphs \emph{chordal} \cite{Dirac:1961}, which means that for every cycle of size $> 3$, there must exist an edge between two non-adjacent nodes in the cycle. In other words, if there exists a cycle in the graph, there must also exist a cycle of at most three nodes. This allows us to encode cycle-freeness much more efficiently by only disallowing cycles of length three.

Apart from the fact that this translation to SAT helps in efficiently (in practice) solving the scheduling problem, there also is another benefit: it is now possible to encode extra constraints on the resulting schedule. We show two \Jeroen{de 2e moet nog uitgewerkt worden} scheduling optimisations that are interesting from an attribute grammar perspective, for which the optimal schedule can be found efficiently by expressing the optimisation in the SAT problem.

This paper therefore makes the following main contributions:
\begin{itemize}
\item We show how to encode the problem of scheduling linear ordered attribute grammars as a SAT-problem
\item We show that chordal graphs can be used to encode cycle-freeness in SAT-problems involving graph encoding
\item We show how certain attribute grammar optimisations can be encode in the SAT-problem
\end{itemize}
Furthermore, we have implement the described techniques in the UUAGC\footnote{\url{http://www.cs.uu.nl/wiki/HUT/AttributeGrammarSystem}} \cite{combinator-languages} and show that the technique works well in practice for the UHC.

The outline of the paper is as follows. We start in \sectref{sect:ag} and \sectref{sect:loag} by explaining linear ordered attribute grammars and the difficulties in scheduling in more detail. We then describe the translation to SAT in \sectref{sect:translation} and show the effectiveness of our approach in \sectref{sect:results}. In \sectref{sect:optimisations} we describe the two attribute grammar optimisations and finally in \sectref{sect:conclusion} we discuss some problems and conclude.

\Jeroen{todo in intro: cite Minisat?}


\section{Attribute Grammars} \label{sect:ag}
Formally an attribute grammar consists of three parts: a context-free grammar, a set of attribute definitions and a set of semantic functions. Instead of a context-free grammar describing the concrete syntax of the language, we feel that it is more intuitive to visualise the grammar as describing the abstract syntax tree of the language. We therefore say that an attribute grammar consists of a set of algebraic data types describing the abstract syntax, a set of attribute definitions and a set of semantic functions. The attribute definitions and semantic functions describe how values should be computed over the abstract syntax tree.

In the rest of this paper we do stick to the usual attribute grammar terminology, so with a \emph{nonterminal} we mean a type, and a \emph{production} is a constructor of that type. Furthermore, the name \emph{lhs} (short for left-hand side) refers to the constructor itself, while the children of a production have separate names. As this paper is about scheduling and data dependencies, we do not explain the syntax of the semantic functions here, but it is important to remark that these functions define how the value of a certain attribute can be computed from other attributes.

There are two types of attributes: \emph{inherited} attributes with values that are passed from a parent to its children, and \emph{synthesized} attributes with values that are passed from the children to their parents. In the pictures we draw inherited attributes on the left side of a node, and the synthesized attributes on its right side.

\mydoublepic{labval_bin_flow_full}{0.55}{labval_leaf_flow_full}{0.3}{
    Example attribute grammar for binary trees with two inherited and two synthesized attributes}

In \figref{fig:labval_bin_flow_full} we show an example attribute grammar with one nonterminal \emph{Tree} which has two productions: \emph{Bin} and \emph{Leaf}. The \emph{Bin} production has two children named \emph{l} and \emph{r}, both of the type \emph{Tree}, and \emph{Leaf} has no children. The \emph{Tree} nonterminal has four attributes: two inherited attributes a1 and a2 and two synthesized attributes a3 and a4.

The figure shows what is called the \emph{production dependency graphs}. For a production two different types of attributes can be distinguished: \emph{input} attributes, which are attributes for which the value is provided, and \emph{output} attributes, which are values that need to be defined. The input attributes of a production, colored gray in the picture, are the inherited attributes of the production itself, because their values are provided by the parent node, and the synthesized attributes of all its children. The output attributes, colored white in the picture, are the synthesized attributes of the parent and the inherited attributes of the child.

Although we talk about dependency graphs in the context of scheduling, we actually draw the edges in the other direction. The edges in the picture denote data flow, which from an attribute grammar perspective is much more intuitive. For example, in the \emph{Bin} production the attribute a1 of the child \emph{r} is computed from a1 from the parent and a3 from the child \emph{l}. The edges are thus always directed from an input attribute to an output attribute, and the true dependency graph can be obtained by reversing all edges.

\section{Linear Ordered Attribute Grammars} \label{sect:loag}
Given an attribute grammar definition the attribute grammar compiler should produce a runtime evaluator that takes an abstract syntax tree as input and computes all attribute values. There are two approaches for doing the scheduling: at runtime (dynamic) or at compile time (static).

For dynamic scheduling one could rely on existing machinery for lazy evaluation, for example in Haskell, which is the approach taken by \cite{saraiva99}. There the attribute grammar definitions are translated to lazy Haskell functions in a straightforward way by producing functions that take the inherited attributes as argument and return the synthesized attributes. Whenever an inherited attribute (indirectly) depends on the value of a synthesized attribute, this means the function has a cyclic definition, which is no problem for languages with lazy semantics, and can actually result in effient code \cite{Bird:1984}.

There are two problems with the dynamic scheduling. First of all whenever the attribute grammar contains a true cyclic attribute definition, the evaluator enters an infinite loop at runtime. However, these cycles could have been detected at compile time! Furthermore, the code needs lazy functions while with the static scheduling strict evaluators can be generated, leading to more effient code in practice.

For static scheduling efficient evaluators can be generated, but the problem is that static scheduling is hard. In \cite{kastens80} a polynomial time algorithm is given for static scheduling of the class of ordered attribute grammars, but unfortunately we often encounter attribute grammars outside of that class. All our attribute grammars do however fall in the class of linear ordered attribute grammars, which is the largest class of attribute grammars for which a static schedule can be found. Although the scheduling is NP-complete, we have implemented a backtracking algorithm in earlier work \cite{Binsbergen:2015} that is feasible in practice. However, in this paper we show that we can do better\Thomas{Faster and allowing for the different optimisations}.

Conceptually, static scheduling of linear ordered attribute grammars is not very complex. For each nonterminal a total order should be found on its attributes, such that all production dependency graphs all cycle free. However, because a nonterminal can have multiple productions and a production can have children of different types, choices made on the order of attributes of one nonterminal can influence the order of attributes in another nonterminal.

In order to encode this we also construct a \emph{nonterminal dependency graph}. This graph contains all attributes that are defined for the corresponding nonterminal and the edges in this graph must define a total order\Thomas{Actually it can be partial, which is also what we obtain using a variable for every inherited x synthesized pair. For example, an interface with only 1 visit (and multiple inherited or synthesized attributes) defines a partial order since we don't care about the order between inherited/synthesized attributes in the same visit. The visit-sequences that are calculated in the end do imply a total order however.} on the attributes. Furthermore, the edges in the nonterminal dependency graph should agree with the (indirect) dependencies from the production dependency graphs such that no cycles exist.

\mypic{labval_bin_flow_induced}{Production dependency graph for Bin for the order a2 $\rightarrow$ a4 $\rightarrow$ a1 $\rightarrow$ a3}

In \figref{fig:labval_bin_flow_induced} we show the production dependency graph of the nonterminal \emph{Bin} with a complete order on the attributes. In this case there is only a single nonterminal so both the parent and the child nodes have the same attribute order, made explicit by extra edges from the nonterminal dependency graph. Because this dependency graph is still cycle free after adding the edges for the complete order and the same holds for the \emph{Leaf} production, the order a2 $\rightarrow$ a4 $\rightarrow$ a1 $\rightarrow$ a3 is a valid schedule for the nonterminal \emph{Tree}.


\section{Translation to SAT} \label{sect:translation}
To represent the scheduling problem as a Boolean formula we introduce a variable for each edge, indicating the direction of the edge. The direct dependencies coming from the source code are constants, but for the rest of the edges the SAT solver can decide on the direction. However, the encoding has been chosen in such way that a valid assigment of the variables corresponds to a valid schedule.

Our algorithm has the following steps:
\begin{enumerate}
\item Construct a nonterminal dependency graph for each nonterminal
\item Construct a production dependency graph for each production
\item Make all graphs chordal
\item Introduce a SAT variable for each edge in any of the graphs, where edges from a production dependency graph and the corresponding nonterminal dependency graph are shared, so are represented by the same variable
\item Set the value of all variables corresponding to direct dependencies
\item Exclude all cycles of length three
\item Optionally add extra constraints for optimisations
\end{enumerate}
The first two steps have been explained in the previous sections. Step 3 is explained below\Thomas{Step 4 is missing}, and step 5 and 6 follow from that. Finally step 7 is explained in \sectref{sect:optimisations}.

\subsection{Chordal graphs}
A chordal graph is an undirected graph in which each cycle of length $> 3$ contains a \emph{chord}. A chord is an edge between two nodes in the cycle that are not adjacent. The result of this is that each cycle of length $> 3$ is split up into two smaller cycles, so if a chordal graph contains a cycle it must also contain a cycle of size three. Chordal graphs are therefore sometimes also referred to as \emph{triangulated graphs}.

\mypic{simplechord}{There can not exists a cycle w-n-e-s without either
a cycle n-s-w or n-e-s.}

In our case the graphs are directed, but we can still apply the same trick! In \figref{fig:simplechord} we illustrate how to use chordal graphs to exclude cycles. If there exists a cycle of length four, then it is not possible to choose a direction for the edge between \emph{n} and \emph{s} without introducing a cycle of length three. Hence, if we make our graph chordal by adding edges which may have an arbitrary direction and explicitly exclude all cycles of length three, we ensure that no cycles can exist at all.

\Jeroen{todo: say something about heuristics for making the graph chordal and how many triangles there are}

\subsection{Finding the schedule}
When the constructed Boolean formula is given to the SAT-solver, the result is either that the formula is not satisfiable, meaning that no schedule has been found, of satisfiable, meaning that there is a schedule. It is not hard to see that the formula is satisfiable if and only if there exists a valid schedule for the given attribute grammar definition, and in this paper we give no formal proof of this claim.

In the case where the formula is satisfiable, we obviously want to find the result. From the SAT solver we can ask for the truth value of each variable in the solution, so when our algorithm keeps the connection between edges and variables we can complete our directed graph and trivially find the complete order for all attributes from that. The constraints guarantee that this graph contains no cycles.

\section{Empirical results} \label{sect:results}
\Jeroen{todo: getallen over UHC laten zien, het werkt echt}

\section{Optimisations} \label{sect:optimisations}
\Jeroen{todo: laten zien dat we totaal aantal visits kunnen minimaliseren en dat we sommige attributen vroeg kunnen schedulen, en intuitie geven waarom dit nuttig is (meer hebben we ook nog niet)}

\section{Conclusion and discussion} \label{sect:conclusion}
\Jeroen{todo: het werkt, misschien is andere representatie beter, en het is onvoorspelbaarder dus misschien duren sommige ags ineens heel lang om te schedulen}


\DeclareRobustCommand{\VAN}[3]{#3}
\bibliographystyle{splncs}
\bibliography{biblio}

\end{document}

% -----------------------------------

\section{Attribute Grammar}
An attribute grammars consists of three parts: a context-free grammar 
describing abstract syntax, a set of inherited and synthesized attributes
assigned to every non-terminal of the grammar and a set of semantic function
definitions. A production rule of a context-free grammar has one non-terminal
as its left-hand side and any number of terminals and non-terminals as its
right-hand side. The non-terminals occurring in production rules are considered
to be non-terminal \emph{occurrences}. A non-terminal occurrence of 
non-terminal $X$ automatically obtains the attributes of $X$ as attribute 
occurrences. The semantic function definitions describe how some attribute 
occurrences of a production rule are calculated in terms of other attribute
occurrences and terminal symbols of that rule. For our purposes we can safely
ignore any terminal symbols. Figure \ref{fig:productions} shows
examples of production rules displayed as diagrams.

\mypic{productions}{Diagrams showing productions, $P1: (lhs:X) \rightarrow 
\alpha$, $P2: (lhs:Y) \rightarrow \alpha (x:X) \beta$, $P3: (lhs:Z)
\rightarrow \alpha (y:Y) \beta (x:X) \gamma$, where $\alpha$, $\beta$,
$\gamma$ are arbitrary terminal symbols.}

All non-terminal occurrences are named, e.g. $(x:X)$ is an occurrence of 
non-terminal $X$ named $x$. Synthesized attributes are shown to the right
and inherited attributes are shown to the left of their respective
non-terminal occurrences. The gray attribute occurrences (inherited in the
left-hand side and synthesized in the right-hand side) are input occurrences,
the others are output occurrences. 

The input occurrences of one production are the output occurrences of another.
Production rules can be `pasted' together to form a tree\cite{knuth68}. 
This task is performed by a parser forming a derivation 
tree\Thomas{Ik stop hier maar even met typen om 
niet te veel uit te wijden, maar eigenlijk zou ik hier nog willen vertellen
dat: 1) Input occurrences van buitenaf worden aangeleverd, 2) dat om dit
te kunnen doen er wel definities voor output occurrences nodig zijn, 
3) dat de pijlen in het figuur (omgekeerde) dependencies weergeven.}.


\section{Linear Ordered Attribute Grammars}
A linear ordered attribute grammar is an attribute grammar for which we can 
find a evaluation order statically\Thomas{Hier zouden we misschien een 
voorbeeld van een parse tree willen laten zien met een interessante 
evaluation order, helaas zijn dat soort plaatjes veel te groot}. 
That is, based on only the information of
non-terminals and productions from the grammar, we can ensure that an 
evaluation order exists for any valid sentence of the grammar.
We show a static evaluation order exists by constructing evaluation orders 
for all production rules such that they satisfy certain properties.
Most importantly: when pasting together production rules to form a 
derivation tree, the evaluation orders for those trees can not contradict 
each other. Figure \ref{fig:contradiction} shows what can go wrong.

\mypic{contradiction}{The evaluation orders depicted by the arrows
of productions $P4$, $P5$ and $P6$ contradict each other: they can
not be combined without forming a cycle.}

Any non-terminal occurrence of $X$ can be replaced by any
of the productions of $X$. Therefore, when constructing an evaluation
order for a production in which $X$ occurs in the right-hand side,
all the evaluation orders constructed for productions in which 
$X$ occurs in the left-hand side need to be taken account.
Changing the evaluation order of one production rule is likely to demand 
a change in the evaluation order of some other production. This is 
what makes LOAG scheduling NP-hard.

In a production graph we can represent the evaluation order of other graphs
using induced dependencies (graph $ID_P$ from Kastens) by adding an 
arrow $a\rightarrow b$ to a non-terminal $X$ if $X$ occurs in some other
production in which there is a path between its $a$ and $b$.
Figure \ref{fig:idp} shows the induced dependency graph of production
$P4$.

\mypic{idp}{The gray edges are induced dependency, showing that they
evaluation orders of Figure \ref{fig:contradiction} are not compatible.}

The induced dependencies will be the same for every occurrence of every
non-terminal and are gathered in induced dependency graphs for 
non-terminals (Kastens $ID_S$).

We can now look at the problem from two directions: 1) we change the
evaluation order of a production and update graph $ID_S$ which might
then affect other production rules and update until all evaluation
orders are compatible\Thomas{AOAG} or 2) we make the decision at non-terminal 
level and decide an order for evaluating the attributes of a non-terminal 
such that an order at production level is still 
possible\Thomas{LOAG, De intuitie achter dit idee zijn de visit-sequences
en intra-visit dependencies, maar die willen we niet noemen}.

\section{Chordal Graphs}
The chords of a chordal graph can be seen as representing the assignment
to the variables `behind' that chord. If there is a chord $a\leftrightarrow b$
then an assignment $a\leftrightarrow^\rightarrow b$ implies that there is 
no path $b\longrightarrow a$ assuming cycle-freeness. This intuition allows
us to see why all assignments that lead to no cycles of length 3 lead to 
no cycles at all in a chordal graph: every cycle of length $> 3$ is 
split by a chord, so one side of the cycles assignments is represented by
the chord in the other.

\mypic{simplechord}{There can not exists a cycle w-n-e-s without either
a cycle n-s-w or n-e-s.}

\Thomas{In mijn scriptie gebruik ik problem graphs en assignment graphs
om de stap van undirected naar directed graphs te maken. Later kwam ik
echter het begrip `orientation' tegen, met hetzelfde doel.}

\section{Definitions}
\input{./loags.tex}

\DeclareRobustCommand{\VAN}[3]{#3}
\bibliographystyle{splncs}
\bibliography{biblio}


\end{document}
